import re
from typing import List
from langchain_core.documents import Document
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_ollama import OllamaLLM

# âš™ï¸ ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° LLM
llm = OllamaLLM(
    model="infidelis/GigaChat-20B-A3B-instruct-v1.5:bf16",
    temperature=0
)

# ðŸ§  ÐŸÑ€Ð¾Ð¼Ð¿Ñ‚ Ð´Ð»Ñ Ð²Ñ‹Ð±Ð¾Ñ€Ð° Ð»ÑƒÑ‡ÑˆÐ¸Ñ… Ñ‡Ð°Ð½ÐºÐ¾Ð²
rerank_prompt = PromptTemplate.from_template(
    """
Ð’Ð¾Ñ‚ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÑÐºÐ¸Ð¹ Ð²Ð¾Ð¿Ñ€Ð¾Ñ:
"{question}"

ÐÐ¸Ð¶Ðµ Ð´Ð°Ð½Ñ‹ Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚Ñ‹ Ñ‚ÐµÐºÑÑ‚Ð° (Ñ ÑƒÐºÐ°Ð·Ð°Ð½Ð¸ÐµÐ¼ Ð½Ð¾Ð¼ÐµÑ€Ð° CHUNK-N):

{context}
Ð’Ñ‹Ð±ÐµÑ€Ð¸ Ð¸ Ð²ÐµÑ€Ð½Ð¸ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ñ‚Ðµ Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚Ñ‹, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ ÑÐ¾Ð´ÐµÑ€Ð¶Ð°Ñ‚ Ñ‚Ð¾Ñ‡Ð½Ñ‹Ðµ Ñ„Ð¾Ñ€Ð¼ÑƒÐ»Ð¸Ñ€Ð¾Ð²ÐºÐ¸ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ð¹ Ð¸Ð· Ð“ÐžÐ¡Ð¢Ð°.
Ð•ÑÐ»Ð¸ ÑÑ€ÐµÐ´Ð¸ Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚Ð¾Ð² ÐµÑÑ‚ÑŒ Ñ„Ð¾Ñ€Ð¼ÑƒÐ»Ð¸Ñ€Ð¾Ð²ÐºÐ° "Ñ‚ÐµÑ€Ð¼Ð¸Ð½: Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ" â€” Ð¾Ð±ÑÐ·Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð²Ñ‹Ð±ÐµÑ€Ð¸ ÐµÑ‘. Ð­Ñ‚Ð¾ Ð¿Ñ€Ð¸Ð¾Ñ€Ð¸Ñ‚ÐµÑ‚Ð½Ñ‹Ð¹ Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚.

"""
)

rerank_chain = rerank_prompt | llm | StrOutputParser()


def rerank_documents(question: str, docs: List[Document], fallback_k: int = 5) -> List[Document]:
    if not docs:
        return []

    combined_text = "\n\n".join([f"[{i+1}] {doc.page_content}" for i, doc in enumerate(docs)])
    selected_text = rerank_chain.invoke({"question": question, "context": combined_text})

    selected_docs = []
    found_chunks = re.findall(r"\[(\d+)\]", selected_text)

    for num in found_chunks:
        idx = int(num) - 1
        if 0 <= idx < len(docs):
            selected_docs.append(docs[idx])

    # ÐŸÑ€Ð¸Ð¾Ñ€Ð¸Ñ‚Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚Ñ‹ Ñ Ñ‚ÐµÑ€Ð¼Ð¸Ð½Ð°Ð¼Ð¸
    selected_docs.sort(key=lambda d: not d.metadata.get("term_chunk", False))

    return selected_docs or docs[:fallback_k]